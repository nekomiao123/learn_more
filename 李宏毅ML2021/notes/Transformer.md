# Transformer

### Encoder

![image-20220326121705633](https://cdn.jsdelivr.net/gh/nekomiao123/pic/img/image-20220326121705633.png)

![image-20220326121724890](https://cdn.jsdelivr.net/gh/nekomiao123/pic/img/image-20220326121724890.png)

![image-20220326122014363](https://cdn.jsdelivr.net/gh/nekomiao123/pic/img/image-20220326122014363.png)

### Decoder

![image-20220327101849149](https://cdn.jsdelivr.net/gh/nekomiao123/pic/img/image-20220327101849149.png)

#### Masked self-attention

![image-20220327102136220](https://cdn.jsdelivr.net/gh/nekomiao123/pic/img/image-20220327102136220.png)

#### AT VS NAT

![image-20220327102408206](https://cdn.jsdelivr.net/gh/nekomiao123/pic/img/image-20220327102408206.png)

#### Cross attention

![image-20220327102442198](https://cdn.jsdelivr.net/gh/nekomiao123/pic/img/image-20220327102442198.png)

### Teacher Forcing

![image-20220327102554298](https://cdn.jsdelivr.net/gh/nekomiao123/pic/img/image-20220327102554298.png)

### Conclusion

![Language Modeling with nn.Transformer and TorchText â€” PyTorch Tutorials  1.11.0+cu102 documentation](https://pytorch.org/tutorials/_images/transformer_architecture.jpg)